{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes contents from [openreview.net](https://openreview.net/) and save them in a SQLite file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLitePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from urllib.parse import urlunparse\n",
    "from scrapy import Spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class SQLitePipeline(object):\n",
    "    \n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        dbpath = getattr(crawler, 'dbpath', None)\n",
    "        return cls(dbpath)\n",
    "    \n",
    "    def __init__(self, dbpath=None):\n",
    "        # SQLite connection\n",
    "        self.dbconn = sqlite3.connect(dbpath or 'openreview.sqlite3')\n",
    "        \n",
    "        # initialize three tables: venue, publication and review\n",
    "        c = self.dbconn.cursor()\n",
    "        c.executescript(\"\"\"\n",
    "        DROP TABLE IF EXISTS venue;\n",
    "        DROP TABLE IF EXISTS publication;\n",
    "        DROP TABLE IF EXISTS review;\n",
    "        CREATE TABLE venue (\n",
    "            id text,\n",
    "            name text            \n",
    "        );\n",
    "        CREATE TABLE publication (\n",
    "            id text,\n",
    "            venue text,\n",
    "            number int,\n",
    "            revisions int, \n",
    "            replycount int,\n",
    "            tcdate int,\n",
    "            tmdate int,\n",
    "            signatures text,\n",
    "            readers text,\n",
    "            writers text,\n",
    "            decision text,\n",
    "            title text,\n",
    "            authors text,\n",
    "            authorids text,\n",
    "            keywords text,\n",
    "            category text,\n",
    "            pdf text,\n",
    "            url text,\n",
    "            paperhash text,\n",
    "            tldr text,\n",
    "            abstract text\n",
    "        );\n",
    "        CREATE TABLE review (\n",
    "            id text,\n",
    "            type text,\n",
    "            replyto text,\n",
    "            venue text, \n",
    "            number int,\n",
    "            tcdate int,\n",
    "            tmdate int,\n",
    "            date text,\n",
    "            signatures text,\n",
    "            readers text,\n",
    "            writers text,\n",
    "            decision text,\n",
    "            rating text,\n",
    "            confidence text,\n",
    "            title text,\n",
    "            review text,\n",
    "            reply text\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"Process item passed from Spider\"\"\"\n",
    "        if item['_type_'] == 'venue':\n",
    "            return self.save('venue',item)\n",
    "        if item['_type_'] == 'review':\n",
    "            return self.save('review',item)\n",
    "        if item['_type_'] == 'publication':\n",
    "            return self.save('publication',item)\n",
    "        if item['_type_'] == 'commit':\n",
    "            return self.dbconn.commit()\n",
    "    \n",
    "    \n",
    "    def save(self, table, info):\n",
    "        cur = self.dbconn.cursor()\n",
    "        keys = [x for x in info.keys() if x != '_type_']\n",
    "        keys_str = ', '.join(keys)\n",
    "        qmarks = ', '.join(['?'] * len(keys))\n",
    "        cur.execute(f\"\"\"\n",
    "        INSERT INTO {table} ({keys_str})\n",
    "        VALUES ({qmarks})\n",
    "        \"\"\", [info[x] for x in keys])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class OpenReviewSpider(Spider):\n",
    "    name = \"OpenReview\"\n",
    "    start_urls = ['https://openreview.net/venues']\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.SQLitePipeline': 300\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(OpenReviewSpider, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        \"\"\"Default parser: the venue list\"\"\"\n",
    "        for name, href in zip(response.css('h3 a::text').extract(),\n",
    "                              response.css('h3 a::attr(\"href\")').extract()):\n",
    "            yield {\n",
    "                '_type_': 'venue',\n",
    "                'id': href.split('id=')[-1],\n",
    "                'name': name\n",
    "            }\n",
    "            yield response.follow(href, callback=self.parse_venue)\n",
    "\n",
    "    def parse_venue(self, response):\n",
    "        \"\"\"Parse submission list in a venue page\"\"\"\n",
    "        for note_id in response.css('.submissions-list li::attr(\"data-id\")').extract():\n",
    "            href = f\"https://openreview.net/notes?forum={note_id}\"\n",
    "            yield response.follow(href, callback=self.parse_submission)\n",
    "\n",
    "    def parse_submission(self, response):\n",
    "        \"\"\"Parse submission detail page. Obtain the submission details\n",
    "        and the reviews\"\"\"\n",
    "        res = json.loads(response.text)['notes']\n",
    "        \n",
    "        def parse_review(item):\n",
    "            venue_id = item['invitation'].split('/-/')[0]\n",
    "            # if 'review' not in item['content']:\n",
    "            #    print(item)\n",
    "            review_type = item['invitation'].split('/-/')[-1]\n",
    "            return {\n",
    "                '_type_': 'review',\n",
    "                'id': item['id'],\n",
    "                'type': review_type,\n",
    "                'venue': venue_id,  # redundancy for better performance\n",
    "                'replyto': item['replyto'],\n",
    "                'number': item['number'],\n",
    "                'tmdate': item['tmdate'],\n",
    "                'tcdate': item['tcdate'],\n",
    "                'signatures': ', '.join(item['signatures']),\n",
    "                'readers': ', '.join(item['readers']),\n",
    "                'writers': ', '.join(item['writers']),\n",
    "                'title': item['content'].get('title'),\n",
    "                'rating': item['content'].get('rating'),\n",
    "                'confidence': item['content'].get('confidence'),\n",
    "                'decision': item['content'].get('decision') or item['content'].get('Acceptance decision'),\n",
    "                'review': item['content'].get('review'),\n",
    "                'reply': item['content'].get('reply'),\n",
    "            }\n",
    "            \n",
    "        def parse_publication(item):\n",
    "            venue_id = item['invitation'].split('/-/')[0]\n",
    "            # if 'paperhash' not in item['content']:\n",
    "            #    print(item)\n",
    "            return {\n",
    "                '_type_': 'publication',\n",
    "                'id': item['id'],\n",
    "                'revisions': item['revisions'],\n",
    "                'venue': venue_id,\n",
    "                'number': item['number'],\n",
    "                'tmdate': item['tmdate'],\n",
    "                'tcdate': item['tcdate'],\n",
    "                'signatures': ', '.join(item['signatures']),\n",
    "                'readers': ', '.join(item['readers']),\n",
    "                'writers': ', '.join(item['writers']),\n",
    "                'title': item['content']['title'],\n",
    "                # decision may not be available if post was withdrawn\n",
    "                'decision': item['content'].get('decision') or item['content'].get('Acceptance decision'),\n",
    "                'tldr': item['content'].get('TL;DR'),\n",
    "                'abstract': item['content']['abstract'],\n",
    "                'pdf': item['content'].get('pdf'),\n",
    "                'category': item['content'].get('submission category'),\n",
    "                'url': item['content'].get('url'),\n",
    "                'paperhash': item['content'].get('paperhash'),\n",
    "                'authors': ', '.join(item['content']['authors']),\n",
    "                'authorids': ', '.join(item['content']['authorids']),\n",
    "                'keywords': ', '.join(item['content'].get('keywords', [])),\n",
    "                'replycount': item['replyCount'],\n",
    "            }\n",
    "        \n",
    "        for item in res:\n",
    "            if '/Comment' in item['invitation']:\n",
    "                continue  # skip comments\n",
    "            if item.get('replyto') is not None:\n",
    "                yield parse_review(item)\n",
    "            else:\n",
    "                yield parse_publication(item)\n",
    "        \n",
    "        # print(f'Finished {item[\"invitation\"].split(\"/-/\")[0]}', end='\\r')\n",
    "        \n",
    "        # after all reviews for this publication is scraped\n",
    "        # do commit\n",
    "        yield {'_type_': 'commit'}\n",
    "                \n",
    " # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-22 10:34:32 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
      "2017-12-22 10:34:32 [scrapy.utils.log] INFO: Overridden settings: {}\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(OpenReviewSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify and some examples\n",
    "\n",
    "Install `ipython-sql`  first.\n",
    "```\n",
    "pip install ipython-sql\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: None@openreview.sqlite3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext sql\n",
    "%sql sqlite:///openreview.sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>n_venue</th>\n",
       "        <th>n_publication</th>\n",
       "        <th>n_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>22</td>\n",
       "        <td>2057</td>\n",
       "        <td>14549</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(22, 2057, 14549)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    (SELECT count(*) from venue) AS n_venue,\n",
    "    (SELECT count(*) from publication) AS n_publication,\n",
    "    (SELECT count(*) from review) AS n_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>AKBC.ws/2013</td>\n",
       "        <td>AKBC 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>cv-foundation.org/CVPR/2017/BNMW</td>\n",
       "        <td>CVPR 2017 BNMW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ECCV2016.org/BNMW</td>\n",
       "        <td>ECCV2016 BNMW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ICLR.cc/2013</td>\n",
       "        <td>ICLR 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>ICLR.cc/2014</td>\n",
       "        <td>ICLR 2014</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('AKBC.ws/2013', 'AKBC 2013'),\n",
       " ('cv-foundation.org/CVPR/2017/BNMW', 'CVPR 2017 BNMW'),\n",
       " ('ECCV2016.org/BNMW', 'ECCV2016 BNMW'),\n",
       " ('ICLR.cc/2013', 'ICLR 2013'),\n",
       " ('ICLR.cc/2014', 'ICLR 2014')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * from venue limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>venue</th>\n",
       "        <th>number</th>\n",
       "        <th>authors</th>\n",
       "        <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>rySGOPjnb</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS</td>\n",
       "        <td>2</td>\n",
       "        <td>withdraw</td>\n",
       "        <td>withdraw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>HkW01LLA-</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS</td>\n",
       "        <td>12</td>\n",
       "        <td>Deepak Mittal, Mudamala Avinash Reddy, Gitakrishnan Ramadurai, Kaushik Mitra, Balaraman Ravindran</td>\n",
       "        <td>Video image processing of traffic camera feeds is useful for counting and classifying vehicles, estimating queue length,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>BypTNXUCW</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS</td>\n",
       "        <td>5</td>\n",
       "        <td>Ramesh​ ​Sarukkai, ​ ​Shaohui​ ​Sun</td>\n",
       "        <td>We present the framework for an unified transport engine that allows for streamlining wide variety of data sources and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>S1uHiFyyg</td>\n",
       "        <td>NIPS.cc/2016/workshop/MLITS</td>\n",
       "        <td>5</td>\n",
       "        <td>Michael Treml, José Arjona-Medina, Thomas Unterthiner, Rupesh Durgesh, Felix Friedmann, Peter Schuberth, Andreas Mayr, Martin Heusel, Markus Hofmarcher, Michael Widrich, Bernhard Nessler, Sepp Hochreiter</td>\n",
       "        <td>Deep learning has considerably improved semantic image segmentation. However,<br>its high accuracy is traded against larger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>HylddmUAZ</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS</td>\n",
       "        <td>6</td>\n",
       "        <td>Mustafa Mukadam, Akansel Cosgun, Alireza Nakhaei, Kikuo Fujimura</td>\n",
       "        <td>In this paper we consider the problem of autonomous lane changing for self driving cars in a multi-lane, multi-agent set...</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('rySGOPjnb', 'NIPS.cc/2017/Workshop/MLITS', 2, 'withdraw', 'withdraw...'),\n",
       " ('HkW01LLA-', 'NIPS.cc/2017/Workshop/MLITS', 12, 'Deepak Mittal, Mudamala Avinash Reddy, Gitakrishnan Ramadurai, Kaushik Mitra, Balaraman Ravindran', 'Video image processing of traffic camera feeds is useful for counting and classifying vehicles, estimating queue length,...'),\n",
       " ('BypTNXUCW', 'NIPS.cc/2017/Workshop/MLITS', 5, 'Ramesh\\u200b \\u200bSarukkai, \\u200b \\u200bShaohui\\u200b \\u200bSun', 'We present the framework for an unified transport engine that allows for streamlining wide variety of data sources and d...'),\n",
       " ('S1uHiFyyg', 'NIPS.cc/2016/workshop/MLITS', 5, 'Michael Treml, José Arjona-Medina, Thomas Unterthiner, Rupesh Durgesh, Felix Friedmann, Peter Schuberth, Andreas Mayr, Martin Heusel, Markus Hofmarcher, Michael Widrich, Bernhard Nessler, Sepp Hochreiter', 'Deep learning has considerably improved semantic image segmentation. However,\\nits high accuracy is traded against larger...'),\n",
       " ('HylddmUAZ', 'NIPS.cc/2017/Workshop/MLITS', 6, 'Mustafa Mukadam, Akansel Cosgun, Alireza Nakhaei, Kikuo Fujimura', 'In this paper we consider the problem of autonomous lane changing for self driving cars in a multi-lane, multi-agent set...')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT id, venue, number, authors, SUBSTR(abstract, 1, 120) || '...' as abstract from publication limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>id</th>\n",
       "        <th>replyto</th>\n",
       "        <th>number</th>\n",
       "        <th>signatures</th>\n",
       "        <th>rating</th>\n",
       "        <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>S1zKiizNZ</td>\n",
       "        <td>Bk_wCFM7b</td>\n",
       "        <td>2</td>\n",
       "        <td>roboticsfoundation.org/RSS/2017/RCW_Workshop/-_Proceedings/Paper4/AnonReviewer4</td>\n",
       "        <td>5: Top 15% of accepted papers, strong accept</td>\n",
       "        <td>+ Good review of compression algorithms, their application to underwater images, along with pros and cons<br>+ Nice descrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>rkPB2MeEW</td>\n",
       "        <td>Bk_wCFM7b</td>\n",
       "        <td>1</td>\n",
       "        <td>roboticsfoundation.org/RSS/2017/RCW_Workshop/-_Proceedings/Paper4/AnonReviewer1</td>\n",
       "        <td>4: Top 50% of accepted papers, clear accept</td>\n",
       "        <td>This submission clearly fits the overall theme of the workshop, and presents sonar image compression and decompression a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>HkYB0n7Jf</td>\n",
       "        <td>rySGOPjnb</td>\n",
       "        <td>3</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS/Paper2/AnonReviewer1</td>\n",
       "        <td>1: Strong rejection</td>\n",
       "        <td>Strengths<br><br>+ The goal is interesting. The paper attempts to address an interesting topic of machine learning research: t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>S1LkoPmyf</td>\n",
       "        <td>rySGOPjnb</td>\n",
       "        <td>2</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS/Paper2/AnonReviewer3</td>\n",
       "        <td>2: Marginally below acceptance threshold</td>\n",
       "        <td>Pros:<br>+ The task solved by the work (traffic speed prediction in time) seems well-motivated<br>+ Predictability of traffic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>H1PzHDmyz</td>\n",
       "        <td>rySGOPjnb</td>\n",
       "        <td>1</td>\n",
       "        <td>NIPS.cc/2017/Workshop/MLITS/Paper2/AnonReviewer2</td>\n",
       "        <td>2: Marginally below acceptance threshold</td>\n",
       "        <td>Quality/Clarity:<br>There are a few things that were confusing on my initial read.<br>- I couldn&#x27;t get a clear image of how sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SJCFQ_vJM</td>\n",
       "        <td>r1tLym8T-</td>\n",
       "        <td>1</td>\n",
       "        <td>NIPS.cc/2017/Workshop/Autodiff/Program_Chairs</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>BJdqn4byG</td>\n",
       "        <td>r1tLym8T-</td>\n",
       "        <td>2</td>\n",
       "        <td>NIPS.cc/2017/Workshop/Autodiff/Paper3/AnonReviewer2</td>\n",
       "        <td>5: Top 15% of accepted papers, strong accept</td>\n",
       "        <td>The author provides an overview of useful, but underutilized, AD techniques. For example, the manuscript highlights:<br>\t- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>H1N0Efbkf</td>\n",
       "        <td>r1tLym8T-</td>\n",
       "        <td>1</td>\n",
       "        <td>NIPS.cc/2017/Workshop/Autodiff/Paper3/AnonReviewer1</td>\n",
       "        <td>4: Top 50% of accepted papers, clear accept</td>\n",
       "        <td>This is a great and clear overview of AD, its usefulness and variants: forward-mode vs. reverse-mode, source-transformat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Hy56Q_DJM</td>\n",
       "        <td>HJ0WtefAW</td>\n",
       "        <td>1</td>\n",
       "        <td>NIPS.cc/2017/Workshop/Autodiff/Program_Chairs</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>SyE4j4-yf</td>\n",
       "        <td>HJ0WtefAW</td>\n",
       "        <td>2</td>\n",
       "        <td>NIPS.cc/2017/Workshop/Autodiff/Paper6/AnonReviewer2</td>\n",
       "        <td>1: Strong rejection</td>\n",
       "        <td>This paper is off-topic for this workshop....</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('S1zKiizNZ', 'Bk_wCFM7b', 2, 'roboticsfoundation.org/RSS/2017/RCW_Workshop/-_Proceedings/Paper4/AnonReviewer4', '5: Top 15% of accepted papers, strong accept', '+ Good review of compression algorithms, their application to underwater images, along with pros and cons\\n+ Nice descrip...'),\n",
       " ('rkPB2MeEW', 'Bk_wCFM7b', 1, 'roboticsfoundation.org/RSS/2017/RCW_Workshop/-_Proceedings/Paper4/AnonReviewer1', '4: Top 50% of accepted papers, clear accept', 'This submission clearly fits the overall theme of the workshop, and presents sonar image compression and decompression a...'),\n",
       " ('HkYB0n7Jf', 'rySGOPjnb', 3, 'NIPS.cc/2017/Workshop/MLITS/Paper2/AnonReviewer1', '1: Strong rejection', 'Strengths\\n\\n+ The goal is interesting. The paper attempts to address an interesting topic of machine learning research: t...'),\n",
       " ('S1LkoPmyf', 'rySGOPjnb', 2, 'NIPS.cc/2017/Workshop/MLITS/Paper2/AnonReviewer3', '2: Marginally below acceptance threshold', 'Pros:\\n+ The task solved by the work (traffic speed prediction in time) seems well-motivated\\n+ Predictability of traffic ...'),\n",
       " ('H1PzHDmyz', 'rySGOPjnb', 1, 'NIPS.cc/2017/Workshop/MLITS/Paper2/AnonReviewer2', '2: Marginally below acceptance threshold', \"Quality/Clarity:\\nThere are a few things that were confusing on my initial read.\\n- I couldn't get a clear image of how sp...\"),\n",
       " ('SJCFQ_vJM', 'r1tLym8T-', 1, 'NIPS.cc/2017/Workshop/Autodiff/Program_Chairs', None, None),\n",
       " ('BJdqn4byG', 'r1tLym8T-', 2, 'NIPS.cc/2017/Workshop/Autodiff/Paper3/AnonReviewer2', '5: Top 15% of accepted papers, strong accept', 'The author provides an overview of useful, but underutilized, AD techniques. For example, the manuscript highlights:\\n\\t- ...'),\n",
       " ('H1N0Efbkf', 'r1tLym8T-', 1, 'NIPS.cc/2017/Workshop/Autodiff/Paper3/AnonReviewer1', '4: Top 50% of accepted papers, clear accept', 'This is a great and clear overview of AD, its usefulness and variants: forward-mode vs. reverse-mode, source-transformat...'),\n",
       " ('Hy56Q_DJM', 'HJ0WtefAW', 1, 'NIPS.cc/2017/Workshop/Autodiff/Program_Chairs', None, None),\n",
       " ('SyE4j4-yf', 'HJ0WtefAW', 2, 'NIPS.cc/2017/Workshop/Autodiff/Paper6/AnonReviewer2', '1: Strong rejection', 'This paper is off-topic for this workshop....')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT id, replyto, number, signatures, rating, SUBSTR(review, 1, 120) || '...' as review from review limit 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
